{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621d77f7-b4ce-4b77-9c39-822b69a6a64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breakfast for the lazy</td>\n",
       "      <td>['put', 'cottage', 'cheese', 'wide', 'bowl', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breek breakfast</td>\n",
       "      <td>['rinse', 'buckwheat', 'pour', 'cup', 'boiling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Childhood breakfast</td>\n",
       "      <td>['grate', 'carrot', 'green', 'apple', 'middle'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>French croutons for breakfast</td>\n",
       "      <td>['mix', 'egg', 'milksaltdip', 'piece', 'loaf',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Low -calorie breakfast</td>\n",
       "      <td>['boil', 'egg', 'boiledcut', 'cheese', 'tomato...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name  \\\n",
       "0         Breakfast for the lazy   \n",
       "1                Breek breakfast   \n",
       "2            Childhood breakfast   \n",
       "3  French croutons for breakfast   \n",
       "4         Low -calorie breakfast   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  ['put', 'cottage', 'cheese', 'wide', 'bowl', '...  \n",
       "1  ['rinse', 'buckwheat', 'pour', 'cup', 'boiling...  \n",
       "2  ['grate', 'carrot', 'green', 'apple', 'middle'...  \n",
       "3  ['mix', 'egg', 'milksaltdip', 'piece', 'loaf',...  \n",
       "4  ['boil', 'egg', 'boiledcut', 'cheese', 'tomato...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gerekli kütüphaneleri yükleyelim\n",
    "import pandas as pd\n",
    "\n",
    "# CSV dosyasını tekrar yükleyelim\n",
    "lemmatized_df = pd.read_csv('lemmatized_data.csv')\n",
    "\n",
    "# Dosyanın ilk 5 satırına bakalım\n",
    "lemmatized_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c84276-c003-42a3-92c4-98358975a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cümle 1 - Lemmatized: ['put', 'cottage', 'cheese', 'wide', 'bowl', 'add', 'egg', 'sugar', 'flour', 'fork', 'mix', 'homogeneous', 'consistencyif', 'mass', 'turned', 'sticky', 'add', 'flour', 'half', 'tablespoonsprinkle', 'table', 'small', 'amount', 'flour', 'lay', 'cottage', 'cheesedivide', 'two', 'equal', 'part', 'roll', 'sausage', 'thickness', 'eachcut', 'sausage', 'small', 'identical', 'piece', 'sharp', 'knifeif', 'desired', 'slightly', 'add', 'piece', 'give', 'rounded', 'shapein', 'small', 'pan', 'bring', 'water', 'boilgently', 'lower', 'dumpling', 'boiling', 'water', 'one', 'onecook', 'stirring', 'slightly', 'slotted', 'spoon', 'dumpling', 'come', 'surface', 'plus', 'another', 'minuteput', 'finished', 'dumpling', 'pan', 'plate', 'pour', 'jam', 'example', 'apricotyou', 'serve', 'hot', 'warm']\n",
      "Cümle 2 - Lemmatized: ['rinse', 'buckwheat', 'pour', 'cup', 'boiling', 'water', 'salt', 'cover', 'cover', 'towelafter', 'minute', 'buckwheat', 'eaten', 'lose', 'nutritional', 'valueit', 'better', 'nightthe', 'required', 'amount', 'wounded', 'buckwheat', 'portioned', 'plate', 'seasoned', 'olive', 'oil', 'soy', 'sauce', 'lemon', 'juice', 'chopped', 'lemon', 'chopped', 'green', 'mixserve', 'chopped', 'vegetable', 'bulgarian', 'pepper', 'carrot', 'pumpkin', 'radish', 'green', 'cocktail']\n",
      "Cümle 3 - Lemmatized: ['grate', 'carrot', 'green', 'apple', 'middle', 'graterremove', 'zest', 'orangesqueeze', 'juice', 'halve', 'cut', 'second', 'small', 'piecesmix', 'apple', 'carrot', 'orange', 'raisin', 'nut', 'favorite', 'season', 'juice', 'honey', 'add', 'cinnamon']\n",
      "Cümle 4 - Lemmatized: ['mix', 'egg', 'milksaltdip', 'piece', 'loaf', 'egg', 'mixture', 'sidesfry', 'vegetable', 'oil', 'side']\n",
      "Cümle 5 - Lemmatized: ['boil', 'egg', 'boiledcut', 'cheese', 'tomatoesmix', 'ingredientssalt', 'taste']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# CSV dosyasını yükle\n",
    "lemmatized_data = pd.read_csv('lemmatized_data.csv')\n",
    "\n",
    "# Lemmatizer'ı başlat\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Stopwords listesini almak\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Kelimeleri tokenleştirip, lemmatize etme\n",
    "def preprocess_sentence(tokens):\n",
    "    # Sadece harf olan kelimeleri al ve stopword'leri çıkar\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]  # Lemmatize etme\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# CSV'deki her cümleyi işleyip lemmatize et\n",
    "tokenized_corpus_lemmatized = []\n",
    "\n",
    "for sentence in lemmatized_data['lemmatized_text']:\n",
    "    lemmatized_tokens = preprocess_sentence(eval(sentence))  # eval kullanarak string formatında listeyi işliyoruz\n",
    "    tokenized_corpus_lemmatized.append(lemmatized_tokens)\n",
    "\n",
    "# İlk 5 cümleyi ve lemmatize edilmiş halleri yazdır\n",
    "for i in range(5):\n",
    "    print(f\"Cümle {i+1} - Lemmatized: {tokenized_corpus_lemmatized[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e486fb4f-d51a-40dc-9b93-643e2516d5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            name  \\\n",
      "0         Breakfast for the lazy   \n",
      "1                Breek breakfast   \n",
      "2            Childhood breakfast   \n",
      "3  French croutons for breakfast   \n",
      "4         Low -calorie breakfast   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  ['put', 'cottage', 'cheese', 'wide', 'bowl', '...  \n",
      "1  ['rinse', 'buckwheat', 'pour', 'cup', 'boiling...  \n",
      "2  ['grate', 'carrot', 'green', 'apple', 'middle'...  \n",
      "3  ['mix', 'egg', 'milksaltdip', 'piece', 'loaf',...  \n",
      "4  ['boil', 'egg', 'boiledcut', 'cheese', 'tomato...  \n"
     ]
    }
   ],
   "source": [
    "# CSV dosyasındaki ilk birkaç satırı kontrol edelim\n",
    "print(lemmatized_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "738c3e79-d348-4802-9310-9619122685b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abaqus  abc  academic  access  accompanying  according  accurately  \\\n",
      "0     0.0  0.0       0.0     0.0           0.0        0.0         0.0   \n",
      "1     0.0  0.0       0.0     0.0           0.0        0.0         0.0   \n",
      "2     0.0  0.0       0.0     0.0           0.0        0.0         0.0   \n",
      "3     0.0  0.0       0.0     0.0           0.0        0.0         0.0   \n",
      "4     0.0  0.0       0.0     0.0           0.0        0.0         0.0   \n",
      "\n",
      "   achieve  acknowledgement  acquiring  ...  wstr   xo   xp  yahoo  year  yet  \\\n",
      "0      0.0              0.0        0.0  ...   0.0  0.0  0.0    0.0   0.0  0.0   \n",
      "1      0.0              0.0        0.0  ...   0.0  0.0  0.0    0.0   0.0  0.0   \n",
      "2      0.0              0.0        0.0  ...   0.0  0.0  0.0    0.0   0.0  0.0   \n",
      "3      0.0              0.0        0.0  ...   0.0  0.0  0.0    0.0   0.0  0.0   \n",
      "4      0.0              0.0        0.0  ...   0.0  0.0  0.0    0.0   0.0  0.0   \n",
      "\n",
      "   yield  yukihiro  zen  zope  \n",
      "0    0.0       0.0  0.0   0.0  \n",
      "1    0.0       0.0  0.0   0.0  \n",
      "2    0.0       0.0  0.0   0.0  \n",
      "3    0.0       0.0  0.0   0.0  \n",
      "4    0.0       0.0  0.0   0.0  \n",
      "\n",
      "[5 rows x 1497 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Lemmatize edilmiş metinlerin listesi, her bir cümleyi tokenlerden tekrar metne çeviriyoruz\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "\n",
    "# TF-IDF vektörleştirici başlatıyoruz\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF matrisini oluşturuyoruz\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_texts)\n",
    "\n",
    "# TF-IDF vektörleştirme işleminde kullanılan tüm kelimelerin eşsiz bir listesini alalım\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# TF-IDF matrisini pandas DataFrame'e çeviriyoruz\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# İlk birkaç satırı gösterelim (ilk 5 cümleyi)\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Açıklama:\n",
    "# Her satır bir cümleyi temsil eder.\n",
    "# Her sütun bir kelimeyi temsil eder.\n",
    "# Hücreler, o kelimenin o cümledeki TF-IDF skorudur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f6f640f-9c51-4ad0-b549-fcb228b24b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\n",
      "dumpling              0.393034\n",
      "apricotyou            0.219230\n",
      "tablespoonsprinkle    0.205215\n",
      "sausage               0.197301\n",
      "eachcut               0.193900\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# CSV dosyasını yükle\n",
    "lemmatized_data = pd.read_csv('lemmatized_data.csv')\n",
    "\n",
    "# Lemmatized metinlerin listesini oluştur (metinler zaten lemmatize edilmiş)\n",
    "lemmatized_texts = [' '.join(eval(sentence)) for sentence in lemmatized_data['lemmatized_text']]\n",
    "\n",
    "# TF-IDF vektörleştirici başlatıyoruz\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF matrisini oluşturuyoruz\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_texts)\n",
    "\n",
    "# TF-IDF vektörleştirme işleminde kullanılan tüm kelimelerin eşsiz bir listesini alalım\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# TF-IDF matrisini pandas DataFrame'e çeviriyoruz\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# İlk cümle için TF-IDF skorlarını al\n",
    "first_sentence_vector = tfidf_df.iloc[0]\n",
    "\n",
    "# Skorlara göre sırala (yüksekten düşüğe)\n",
    "top_5_words = first_sentence_vector.sort_values(ascending=False).head(5)\n",
    "\n",
    "# Sonucu yazdır\n",
    "print(\"İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\")\n",
    "print(top_5_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47415005-73d9-418b-9727-971ad770e2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'egg' kelimesine en çok benzeyen kelimeler:\n",
      "egg: 1.0000\n",
      "flour: 0.4003\n",
      "dough: 0.3146\n",
      "beat: 0.3025\n",
      "sugar: 0.2991\n",
      "add: 0.2926\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "target_word = 'egg'\n",
    "\n",
    "if target_word in feature_names:\n",
    "    egg_index = feature_names.tolist().index(target_word)\n",
    "    \n",
    "    # Kelime vektörünü al (tüm belgelerdeki TF-IDF değerleri)\n",
    "    egg_vector = tfidf_matrix[:, egg_index]  # Shape: (num_docs, 1)\n",
    "    \n",
    "    # Tüm kelime vektörlerini al (transpose ederek)\n",
    "    word_vectors = tfidf_matrix.T  # Shape: (num_terms, num_docs)\n",
    "    \n",
    "    # 'egg' kelimesi ile diğer tüm kelimeler arasındaki benzerliği hesapla\n",
    "    similarities = cosine_similarity(word_vectors[egg_index], word_vectors).flatten()\n",
    "    \n",
    "    # En benzer 5 kelimeyi bul (kendisi de dahil olacağı için 6 tane alıyoruz)\n",
    "    top_5_indices = similarities.argsort()[-6:][::-1]\n",
    "    \n",
    "    print(f\"'{target_word}' kelimesine en çok benzeyen kelimeler:\")\n",
    "    for index in top_5_indices:\n",
    "        print(f\"{feature_names[index]}: {similarities[index]:.4f}\")\n",
    "else:\n",
    "    print(f\"'{target_word}' kelimesi veri setinde bulunamadı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f1f9ea6-5323-4279-b663-ec3c8ed461e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Küçük CSV dosyası başarıyla kaydedildi!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# CSV dosyasını yükle\n",
    "lemmatized_data = pd.read_csv('lemmatized_data.csv')\n",
    "\n",
    "# Lemmatized metin listesini oluştur\n",
    "lemmatized_texts = [' '.join(eval(sentence)) for sentence in lemmatized_data['lemmatized_text']]\n",
    "\n",
    "# İlk 5000 cümleyi kullan (az veri için)\n",
    "lemmatized_texts_sample = lemmatized_texts[:5000]\n",
    "\n",
    "# TF-IDF vektörleştirici başlat\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_texts_sample)\n",
    "\n",
    "# Özellik isimlerini al\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# DataFrame'e çevir\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# CSV olarak kaydet\n",
    "df_tfidf.to_csv('tfidf_lemmatized_sample.csv', index=False)\n",
    "\n",
    "print(\"Küçük CSV dosyası başarıyla kaydedildi!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb7c99-7e7c-40a0-9a27-2b2a9b6189c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
