{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440df451-4be6-4e34-bb18-c8244f3d4de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lemmatized_sentences</th>\n",
       "      <th>ingredient_lemmatized_sentences</th>\n",
       "      <th>name_lemmatized_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>put cottage cheese wide bowl add egg sugar flo...</td>\n",
       "      <td>chicken egg piece soft cottage cheese g wheat ...</td>\n",
       "      <td>breakfast lazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rinse buckwheat pour cup boiling water salt co...</td>\n",
       "      <td>buckwheat cereal cup chopped parsley taste cho...</td>\n",
       "      <td>breek breakfast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grate carrot green apple middle zest juice hal...</td>\n",
       "      <td>carrot piece apple piece orange piece raisin g...</td>\n",
       "      <td>childhood breakfast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mix egg piece loaf egg mixture vegetable oil side</td>\n",
       "      <td>baton piece milk tablespoon chicken egg piece ...</td>\n",
       "      <td>french crouton breakfast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boil egg cheese taste</td>\n",
       "      <td>green salad bundle chicken egg piece tomato pi...</td>\n",
       "      <td>low breakfast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           text_lemmatized_sentences  \\\n",
       "0  put cottage cheese wide bowl add egg sugar flo...   \n",
       "1  rinse buckwheat pour cup boiling water salt co...   \n",
       "2  grate carrot green apple middle zest juice hal...   \n",
       "3  mix egg piece loaf egg mixture vegetable oil side   \n",
       "4                              boil egg cheese taste   \n",
       "\n",
       "                     ingredient_lemmatized_sentences name_lemmatized_sentences  \n",
       "0  chicken egg piece soft cottage cheese g wheat ...            breakfast lazy  \n",
       "1  buckwheat cereal cup chopped parsley taste cho...           breek breakfast  \n",
       "2  carrot piece apple piece orange piece raisin g...       childhood breakfast  \n",
       "3  baton piece milk tablespoon chicken egg piece ...  french crouton breakfast  \n",
       "4  green salad bundle chicken egg piece tomato pi...             low breakfast  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gerekli kütüphaneleri yükleyelim\n",
    "import pandas as pd\n",
    "\n",
    "# CSV dosyasını tekrar yükleyelim\n",
    "lemmatized_df = pd.read_csv('lemmatized_data.csv')\n",
    "\n",
    "# Dosyanın ilk 5 satırına bakalım\n",
    "lemmatized_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfbdeb2e-f47a-4b07-95b5-939d96827664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cümle 1 - Lemmatized: ['put', 'cottage', 'cheese', 'wide', 'bowl', 'add', 'egg', 'sugar', 'flour', 'fork', 'mix', 'homogeneous', 'mass', 'turned', 'sticky', 'add', 'flour', 'half', 'table', 'small', 'amount', 'flour', 'lay', 'cottage', 'two', 'equal', 'part', 'roll', 'sausage', 'thickness', 'sausage', 'small', 'identical', 'piece', 'sharp', 'desired', 'slightly', 'add', 'piece', 'give', 'rounded', 'small', 'pan', 'bring', 'water', 'lower', 'dumpling', 'boiling', 'water', 'one', 'stirring', 'slightly', 'slotted', 'spoon', 'dumpling', 'come', 'surface', 'plus', 'another', 'finished', 'dumpling', 'pan', 'plate', 'pour', 'jam', 'example', 'serve', 'hot', 'warm']\n",
      "Cümle 2 - Lemmatized: ['rinse', 'buckwheat', 'pour', 'cup', 'boiling', 'water', 'salt', 'cover', 'cover', 'minute', 'buckwheat', 'eaten', 'lose', 'nutritional', 'better', 'required', 'amount', 'wounded', 'buckwheat', 'portioned', 'plate', 'seasoned', 'olive', 'oil', 'soy', 'sauce', 'lemon', 'juice', 'chopped', 'lemon', 'chopped', 'green', 'chopped', 'vegetable', 'bulgarian', 'pepper', 'carrot', 'pumpkin', 'radish', 'green', 'cocktail']\n",
      "Cümle 3 - Lemmatized: ['grate', 'carrot', 'green', 'apple', 'middle', 'zest', 'juice', 'halve', 'cut', 'second', 'small', 'apple', 'carrot', 'orange', 'raisin', 'nut', 'favorite', 'season', 'juice', 'honey', 'add', 'cinnamon']\n",
      "Cümle 4 - Lemmatized: ['mix', 'egg', 'piece', 'loaf', 'egg', 'mixture', 'vegetable', 'oil', 'side']\n",
      "Cümle 5 - Lemmatized: ['boil', 'egg', 'cheese', 'taste']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# CSV dosyasını yükle\n",
    "lemmatized_data = pd.read_csv('lemmatized_data.csv')\n",
    "\n",
    "# Lemmatizer ve stopword listesi\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Ön işleme fonksiyonu\n",
    "def preprocess_lemmatized_sentence(sentence):\n",
    "    tokens = sentence.split()  # Boşluklara göre kelimeleri ayır\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Her satırı işle\n",
    "tokenized_corpus_lemmatized = []\n",
    "\n",
    "for sentence in lemmatized_data['text_lemmatized_sentences']:\n",
    "    try:\n",
    "        lemmatized_tokens = preprocess_lemmatized_sentence(sentence)\n",
    "        tokenized_corpus_lemmatized.append(lemmatized_tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Hata oluştu: {e}\")\n",
    "        tokenized_corpus_lemmatized.append([])\n",
    "\n",
    "# İlk 5 sonucu yazdır\n",
    "for i in range(5):\n",
    "    print(f\"Cümle {i+1} - Lemmatized: {tokenized_corpus_lemmatized[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e9aa767-1d11-4f3e-a9fa-93034cf913c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           text_lemmatized_sentences  \\\n",
      "0  put cottage cheese wide bowl add egg sugar flo...   \n",
      "1  rinse buckwheat pour cup boiling water salt co...   \n",
      "2  grate carrot green apple middle zest juice hal...   \n",
      "3  mix egg piece loaf egg mixture vegetable oil side   \n",
      "4                              boil egg cheese taste   \n",
      "\n",
      "                     ingredient_lemmatized_sentences name_lemmatized_sentences  \n",
      "0  chicken egg piece soft cottage cheese g wheat ...            breakfast lazy  \n",
      "1  buckwheat cereal cup chopped parsley taste cho...           breek breakfast  \n",
      "2  carrot piece apple piece orange piece raisin g...       childhood breakfast  \n",
      "3  baton piece milk tablespoon chicken egg piece ...  french crouton breakfast  \n",
      "4  green salad bundle chicken egg piece tomato pi...             low breakfast  \n"
     ]
    }
   ],
   "source": [
    "# CSV dosyasındaki ilk birkaç satırı kontrol edelim\n",
    "print(lemmatized_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7936eb-a6df-45f9-bd81-ddf61fc51ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abandon  abandoned  abc  abdomen  abdominal  abelus  ability  abkhaz  \\\n",
      "0        0          0    0        0          0       0        0       0   \n",
      "1        0          0    0        0          0       0        0       0   \n",
      "2        0          0    0        0          0       0        0       0   \n",
      "3        0          0    0        0          0       0        0       0   \n",
      "4        0          0    0        0          0       0        0       0   \n",
      "\n",
      "   abkhazian  able  ...  zukata  zuko  zvezda  zwilling  zyren  ºc  ñora  \\\n",
      "0          0     0  ...       0     0       0         0      0   0     0   \n",
      "1          0     0  ...       0     0       0         0      0   0     0   \n",
      "2          0     0  ...       0     0       0         0      0   0     0   \n",
      "3          0     0  ...       0     0       0         0      0   0     0   \n",
      "4          0     0  ...       0     0       0         0      0   0     0   \n",
      "\n",
      "   λάχανο  ρύζι  ᵒs  \n",
      "0       0     0   0  \n",
      "1       0     0   0  \n",
      "2       0     0   0  \n",
      "3       0     0   0  \n",
      "4       0     0   0  \n",
      "\n",
      "[5 rows x 14258 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Lemmatized edilmiş metinlerin listesi: her bir cümleyi token'lerden tekrar metne çeviriyoruz\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "\n",
    "# TF-IDF vektörleştirici başlatılıyor\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF matrisini oluşturuyoruz (sparse matrix formatında)\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_texts)\n",
    "\n",
    "# Sparse matrisi pandas DataFrame'e dönüştürme\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# İlk 5 satırı gösterelim\n",
    "print(tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22db14ed-9eed-48ee-94ff-e59f7b0b3d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['put cottage cheese wide bowl add egg sugar flour fork mix homogeneous mass turned sticky add flour half table small amount flour lay cottage two equal part roll sausage thickness sausage small identical piece sharp desired slightly add piece give rounded small pan bring water lower dumpling boiling water one stirring slightly slotted spoon dumpling come surface plus another finished dumpling pan plate pour jam example serve hot warm', 'rinse buckwheat pour cup boiling water salt cover cover minute buckwheat eaten lose nutritional better required amount wounded buckwheat portioned plate seasoned olive oil soy sauce lemon juice chopped lemon chopped green chopped vegetable bulgarian pepper carrot pumpkin radish green cocktail', 'grate carrot green apple middle zest juice halve cut second small apple carrot orange raisin nut favorite season juice honey add cinnamon']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ön işlenmiş lemmatized token listelerini tekrar metne çeviriyoruz\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "\n",
    "# İlk 3 lemmatized metni yazdıralım\n",
    "print(lemmatized_texts[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a54ce0-d1dc-4dc2-835c-a9c71918096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abandon  abandoned  abc  abdomen  abdominal  abelus  ability  abkhaz  \\\n",
      "0      0.0        0.0  0.0      0.0        0.0     0.0      0.0     0.0   \n",
      "1      0.0        0.0  0.0      0.0        0.0     0.0      0.0     0.0   \n",
      "2      0.0        0.0  0.0      0.0        0.0     0.0      0.0     0.0   \n",
      "3      0.0        0.0  0.0      0.0        0.0     0.0      0.0     0.0   \n",
      "4      0.0        0.0  0.0      0.0        0.0     0.0      0.0     0.0   \n",
      "\n",
      "   abkhazian  able  ...  zukata  zuko  zvezda  zwilling  zyren   ºc  ñora  \\\n",
      "0        0.0   0.0  ...     0.0   0.0     0.0       0.0    0.0  0.0   0.0   \n",
      "1        0.0   0.0  ...     0.0   0.0     0.0       0.0    0.0  0.0   0.0   \n",
      "2        0.0   0.0  ...     0.0   0.0     0.0       0.0    0.0  0.0   0.0   \n",
      "3        0.0   0.0  ...     0.0   0.0     0.0       0.0    0.0  0.0   0.0   \n",
      "4        0.0   0.0  ...     0.0   0.0     0.0       0.0    0.0  0.0   0.0   \n",
      "\n",
      "   λάχανο  ρύζι   ᵒs  \n",
      "0     0.0   0.0  0.0  \n",
      "1     0.0   0.0  0.0  \n",
      "2     0.0   0.0  0.0  \n",
      "3     0.0   0.0  0.0  \n",
      "4     0.0   0.0  0.0  \n",
      "\n",
      "[5 rows x 14258 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Lemmatize edilmiş token listelerini tekrar düz metne çeviriyoruz\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "\n",
    "# TF-IDF vektörizer'ı başlatıyoruz\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF matrisini oluşturuyoruz\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_texts)\n",
    "\n",
    "# TF-IDF işleminde kullanılan tüm kelimelerin eşsiz bir listesini alıyoruz\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# TF-IDF matrisini pandas DataFrame'e çeviriyoruz\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# İlk birkaç satırı gösteriyoruz\n",
    "print(tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1cbeb9-6b98-42d8-9c81-cdba9f159895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime (lemmatized):\n",
      "dumpling    0.486463\n",
      "sausage     0.243349\n",
      "cottage     0.210088\n",
      "rounded     0.209598\n",
      "plus        0.196422\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# İlk cümle için TF-IDF skorlarını alıyoruz (lemmatized versiyon)\n",
    "first_sentence_vector_lemmatized = tfidf_df.iloc[0]\n",
    "\n",
    "# Skorlara göre büyükten küçüğe sıralayıp ilk 5 kelimeyi alıyoruz\n",
    "top_5_words_lemmatized = first_sentence_vector_lemmatized.sort_values(ascending=False).head(5)\n",
    "\n",
    "# Sonuçları yazdırıyoruz\n",
    "print(\"İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime (lemmatized):\")\n",
    "print(top_5_words_lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae6d7bbc-9a03-4dc0-a777-04a0bbbefd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'potato' kelimesine en yakın 5 kelime (cosine similarity - lemmatized):\n",
      "potato: 1.0000\n",
      "mashed: 0.3138\n",
      "onion: 0.2535\n",
      "cut: 0.2477\n",
      "salt: 0.2351\n",
      "boil: 0.2285\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Hedef kelimeyi belirleyelim (örnek: 'potato')\n",
    "target_word = 'potato'\n",
    "\n",
    "# Kelimenin TF-IDF özellik isimleri arasında olup olmadığını kontrol ediyoruz\n",
    "if target_word in feature_names:\n",
    "    target_index = feature_names.tolist().index(target_word)\n",
    "\n",
    "    # Hedef kelimenin TF-IDF vektörünü (sütun olarak) alıyoruz\n",
    "    target_vector = tfidf_matrix[:, target_index].toarray()\n",
    "\n",
    "    # Tüm kelimelerin TF-IDF vektörlerini (her sütun bir kelime olacak şekilde) alıyoruz\n",
    "    tfidf_vectors = tfidf_matrix.toarray()\n",
    "\n",
    "    # Cosine similarity hesaplıyoruz\n",
    "    similarities = cosine_similarity(target_vector.T, tfidf_vectors.T)\n",
    "\n",
    "    # Sonuçları sıralayıp ilk 6 tanesini (kendisi + 5 en benzeri) alıyoruz\n",
    "    similarities = similarities.flatten()\n",
    "    top_6_indices = similarities.argsort()[-6:][::-1]\n",
    "\n",
    "    # Sonuçları yazdırıyoruz\n",
    "    print(f\"'{target_word}' kelimesine en yakın 5 kelime (cosine similarity - lemmatized):\")\n",
    "    for idx in top_6_indices:\n",
    "        print(f\"{feature_names[idx]}: {similarities[idx]:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"'{target_word}' kelimesi TF-IDF kelime listesinde bulunamadı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb79bef-46ae-4a6b-9e81-26327e3515ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrisi başarıyla 'tfidf_lemmatized.csv' dosyasına kaydedildi!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Lemmatized token listesi zaten bellekte varsa onu kullanalım\n",
    "# Örneğin: tokenized_corpus_lemmatized gibi bir değişkenin olduğunu varsayıyoruz\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "\n",
    "# TF-IDF vektörleştirici oluşturuluyor\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_texts)\n",
    "\n",
    "# Özellik (kelime) isimleri alınıyor\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# TF-IDF matrisi DataFrame'e dönüştürülüyor\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# CSV dosyasına kaydediliyor\n",
    "df_tfidf.to_csv('tfidf_matrix_lemmatized.csv', index=False)\n",
    "\n",
    "print(\"TF-IDF matrisi başarıyla 'tfidf_lemmatized.csv' dosyasına kaydedildi!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baab94e-5358-49fe-b340-096e1595241a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
