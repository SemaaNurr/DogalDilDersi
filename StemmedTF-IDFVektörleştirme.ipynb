{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a40a76-86a9-41be-a45c-19e7a8d6e039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>stemmed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breakfast for the lazy</td>\n",
       "      <td>['put', 'cottag', 'chees', 'wide', 'bowl', 'ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breek breakfast</td>\n",
       "      <td>['rins', 'buckwheat', 'pour', 'cup', 'boil', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Childhood breakfast</td>\n",
       "      <td>['grate', 'carrot', 'green', 'appl', 'middl', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>French croutons for breakfast</td>\n",
       "      <td>['mix', 'egg', 'milksaltdip', 'piec', 'loaf', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Low -calorie breakfast</td>\n",
       "      <td>['boil', 'egg', 'boiledcut', 'chees', 'tomatoe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name  \\\n",
       "0         Breakfast for the lazy   \n",
       "1                Breek breakfast   \n",
       "2            Childhood breakfast   \n",
       "3  French croutons for breakfast   \n",
       "4         Low -calorie breakfast   \n",
       "\n",
       "                                        stemmed_text  \n",
       "0  ['put', 'cottag', 'chees', 'wide', 'bowl', 'ad...  \n",
       "1  ['rins', 'buckwheat', 'pour', 'cup', 'boil', '...  \n",
       "2  ['grate', 'carrot', 'green', 'appl', 'middl', ...  \n",
       "3  ['mix', 'egg', 'milksaltdip', 'piec', 'loaf', ...  \n",
       "4  ['boil', 'egg', 'boiledcut', 'chees', 'tomatoe...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gerekli kütüphaneleri yükleyelim\n",
    "import pandas as pd\n",
    "\n",
    "# CSV dosyasını tekrar yükleyelim\n",
    "stemmed_df = pd.read_csv('stemmed_data.csv')\n",
    "\n",
    "# Dosyanın ilk 5 satırına bakalım\n",
    "stemmed_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f1c0f8-96b1-48be-8e31-ecd0073a2a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cümle 1 - Stemmed: ['put', 'cottag', 'chee', 'wide', 'bowl', 'add', 'egg', 'sugar', 'flour', 'fork', 'mix', 'homogen', 'consistencyif', 'mass', 'turn', 'sticki', 'add', 'flour', 'half', 'tablespoonsprinkl', 'tabl', 'small', 'amount', 'flour', 'lay', 'cottag', 'cheesedivid', 'two', 'equal', 'part', 'roll', 'sausag', 'thick', 'eachcut', 'sausag', 'small', 'ident', 'piec', 'sharp', 'knifeif', 'desir', 'slightli', 'add', 'piec', 'give', 'round', 'shapein', 'small', 'pan', 'bring', 'water', 'boilgent', 'lower', 'dumpl', 'boil', 'water', 'one', 'onecook', 'stir', 'slightli', 'slot', 'spoon', 'dumpl', 'come', 'surfac', 'plu', 'anoth', 'minuteput', 'finish', 'dumpl', 'pan', 'plate', 'pour', 'jam', 'exampl', 'apricoti', 'serv', 'hot', 'warm']\n",
      "Cümle 2 - Stemmed: ['rin', 'buckwheat', 'pour', 'cup', 'boil', 'water', 'salt', 'cover', 'cover', 'towelaft', 'minut', 'buckwheat', 'eaten', 'lose', 'nutrit', 'valueit', 'better', 'nightth', 'requir', 'amount', 'wound', 'buckwheat', 'portion', 'plate', 'season', 'oliv', 'oil', 'soy', 'sauc', 'lemon', 'juic', 'chop', 'lemon', 'chop', 'green', 'mixserv', 'chop', 'veget', 'bulgarian', 'pepper', 'carrot', 'pumpkin', 'radish', 'green', 'cocktail']\n",
      "Cümle 3 - Stemmed: ['grate', 'carrot', 'green', 'appl', 'middl', 'graterremov', 'zest', 'orangesqueez', 'juic', 'halv', 'cut', 'second', 'small', 'piecesmix', 'appl', 'carrot', 'orang', 'raisin', 'nut', 'favorit', 'season', 'juic', 'honey', 'add', 'cinnamon']\n",
      "Cümle 4 - Stemmed: ['mix', 'egg', 'milksaltdip', 'piec', 'loaf', 'egg', 'mixtur', 'sidesfri', 'veget', 'oil', 'side']\n",
      "Cümle 5 - Stemmed: ['boil', 'egg', 'boiledcut', 'chee', 'tomatoesmix', 'ingredientssalt', 'tast']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# CSV dosyasını yükle\n",
    "stemmed_data = pd.read_csv('stemmed_data.csv')\n",
    "\n",
    "# Stemmer'ı başlat\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stopwords listesini almak\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Kelimeleri tokenleştirip, stem etme\n",
    "def preprocess_sentence(tokens):\n",
    "    # Sadece harf olan kelimeleri al ve stopword'leri çıkar\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]  # Stem etme\n",
    "    return stemmed_tokens\n",
    "\n",
    "# CSV'deki her cümleyi işleyip stemmed et\n",
    "tokenized_corpus_stemmed = []\n",
    "\n",
    "for sentence in stemmed_data['stemmed_text']:\n",
    "    stemmed_tokens = preprocess_sentence(eval(sentence))  # eval kullanarak string formatında listeyi işliyoruz\n",
    "    tokenized_corpus_stemmed.append(stemmed_tokens)\n",
    "\n",
    "# İlk 5 cümleyi ve stemmed edilmiş halleri yazdır\n",
    "for i in range(5):\n",
    "    print(f\"Cümle {i+1} - Stemmed: {tokenized_corpus_stemmed[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04f30d52-c77d-4afb-86a2-9fe3e6a5e504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            name  \\\n",
      "0         Breakfast for the lazy   \n",
      "1                Breek breakfast   \n",
      "2            Childhood breakfast   \n",
      "3  French croutons for breakfast   \n",
      "4         Low -calorie breakfast   \n",
      "\n",
      "                                        stemmed_text  \n",
      "0  ['put', 'cottag', 'chees', 'wide', 'bowl', 'ad...  \n",
      "1  ['rins', 'buckwheat', 'pour', 'cup', 'boil', '...  \n",
      "2  ['grate', 'carrot', 'green', 'appl', 'middl', ...  \n",
      "3  ['mix', 'egg', 'milksaltdip', 'piec', 'loaf', ...  \n",
      "4  ['boil', 'egg', 'boiledcut', 'chees', 'tomatoe...  \n"
     ]
    }
   ],
   "source": [
    "# CSV dosyasındaki ilk birkaç satırı kontrol edelim\n",
    "print(stemmed_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c366e03-d017-4074-8b07-dba3f327052b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abandon  abandonedmix  abc  abdomen  abdomencut  abdomengr  abdomengrind  \\\n",
      "0        0             0    0        0           0          0             0   \n",
      "1        0             0    0        0           0          0             0   \n",
      "2        0             0    0        0           0          0             0   \n",
      "3        0             0    0        0           0          0             0   \n",
      "4        0             0    0        0           0          0             0   \n",
      "\n",
      "   abdomenheat  abdomenin  abdomenmak  ...  zucketsraisinsdri  zucketsreadi  \\\n",
      "0            0          0           0  ...                  0             0   \n",
      "1            0          0           0  ...                  0             0   \n",
      "2            0          0           0  ...                  0             0   \n",
      "3            0          0           0  ...                  0             0   \n",
      "4            0          0           0  ...                  0             0   \n",
      "\n",
      "   zucketssprinkl  zukata  zuko  zurbalishchop  zurbalishmix  zvezda  zwill  \\\n",
      "0               0       0     0              0             0       0      0   \n",
      "1               0       0     0              0             0       0      0   \n",
      "2               0       0     0              0             0       0      0   \n",
      "3               0       0     0              0             0       0      0   \n",
      "4               0       0     0              0             0       0      0   \n",
      "\n",
      "   zyren  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "\n",
      "[5 rows x 87273 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Stemmed edilmiş metinlerin listesi, her bir cümleyi tokenlerden tekrar metne çeviriyoruz\n",
    "stemmed_texts = [' '.join(tokens) for tokens in tokenized_corpus_stemmed]\n",
    "\n",
    "# TF-IDF vektörleştirici başlatıyoruz\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF matrisini oluşturuyoruz (sparse matrix)\n",
    "tfidf_matrix = vectorizer.fit_transform(stemmed_texts)\n",
    "\n",
    "# Sparse matrisi pandas DataFrame'e dönüştürme\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# İlk birkaç satırı gösterelim (ilk 5 cümleyi)\n",
    "print(tfidf_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf1d2640-fd85-4a07-8fe7-f4804b666c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\n",
      "dumpl                0.401239\n",
      "apricoty             0.223807\n",
      "tablespoonsprinkl    0.209499\n",
      "sausag               0.201421\n",
      "eachcut              0.197948\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# CSV dosyasını yükle\n",
    "stemmed_data = pd.read_csv('stemmed_data.csv')\n",
    "\n",
    "# Lemmatized metinlerin listesini oluştur (metinler zaten lemmatize edilmiş)\n",
    "stemmed_texts = [' '.join(eval(sentence)) for sentence in stemmed_data['stemmed_text']]\n",
    "\n",
    "# TF-IDF vektörleştirici başlatıyoruz\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF matrisini oluşturuyoruz\n",
    "tfidf_matrix = vectorizer.fit_transform(stemmed_texts)\n",
    "\n",
    "# İlk cümle için TF-IDF skorlarını almak\n",
    "first_sentence_vector = tfidf_matrix[0].toarray().flatten()\n",
    "\n",
    "# Skorlara göre sırala (yüksekten düşüğe)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_5_words = pd.Series(first_sentence_vector, index=feature_names).sort_values(ascending=False).head(5)\n",
    "\n",
    "# Sonucu yazdır\n",
    "print(\"İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\")\n",
    "print(top_5_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbdcca42-2aa2-415a-8928-ce06490ada4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'egg' kelimesine en çok benzeyen kelimeler:\n",
      "egg: 1.0000\n",
      "flour: 0.4003\n",
      "dough: 0.3138\n",
      "beat: 0.3024\n",
      "sugar: 0.2992\n",
      "bake: 0.2951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "target_word = 'egg'\n",
    "\n",
    "if target_word in feature_names:\n",
    "    egg_index = feature_names.tolist().index(target_word)\n",
    "    \n",
    "    # Kelime vektörünü al (tüm belgelerdeki TF-IDF değerleri)\n",
    "    egg_vector = tfidf_matrix[:, egg_index]  # Shape: (num_docs, 1)\n",
    "    \n",
    "    # Tüm kelime vektörlerini al (transpose ederek)\n",
    "    word_vectors = tfidf_matrix.T  # Shape: (num_terms, num_docs)\n",
    "    \n",
    "    # 'egg' kelimesi ile diğer tüm kelimeler arasındaki benzerliği hesapla\n",
    "    similarities = cosine_similarity(word_vectors[egg_index], word_vectors).flatten()\n",
    "    \n",
    "    # En benzer 5 kelimeyi bul (kendisi de dahil olacağı için 6 tane alıyoruz)\n",
    "    top_5_indices = similarities.argsort()[-6:][::-1]\n",
    "    \n",
    "    print(f\"'{target_word}' kelimesine en çok benzeyen kelimeler:\")\n",
    "    for index in top_5_indices:\n",
    "        print(f\"{feature_names[index]}: {similarities[index]:.4f}\")\n",
    "else:\n",
    "    print(f\"'{target_word}' kelimesi veri setinde bulunamadı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9400372-2d99-4a94-ad16-3edecebd43e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk 5000 TF-IDF verisi başarıyla tfidf_stemmed_5000.csv dosyasına kaydedildi!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# CSV dosyasını yükle (örnek: stem yapılmış metinler)\n",
    "stemmed_data = pd.read_csv('stemmed_data.csv')\n",
    "\n",
    "# Metin listesini oluştur\n",
    "stemmed_texts = [' '.join(eval(sentence)) for sentence in stemmed_data['stemmed_text']]\n",
    "\n",
    "# TF-IDF vektörleştirici\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(stemmed_texts)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# İlk 5000 satır için DataFrame oluştur\n",
    "small_df = pd.DataFrame(tfidf_matrix[:5000].toarray(), columns=feature_names)\n",
    "\n",
    "# CSV olarak kaydet\n",
    "small_df.to_csv('tfidf_stemmed_5000.csv', index=False)\n",
    "\n",
    "print(\"İlk 5000 TF-IDF verisi başarıyla tfidf_stemmed_5000.csv dosyasına kaydedildi!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3dc59-7b81-4750-93e1-aea34a70a1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
