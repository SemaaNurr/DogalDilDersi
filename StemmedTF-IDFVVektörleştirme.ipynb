{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4076d8e-8f94-4c4a-b7a9-d1718ad25f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_stemmed_sentences</th>\n",
       "      <th>ingredient_stemmed_sentences</th>\n",
       "      <th>name_stemmed_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>put cottag chees wide bowl add egg sugar flour...</td>\n",
       "      <td>chicken egg piec soft cottag chees g wheat flo...</td>\n",
       "      <td>breakfast lazi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rins buckwheat pour cup boil water salt cover ...</td>\n",
       "      <td>buckwheat cereal cup chop parsley tast chop ci...</td>\n",
       "      <td>breek breakfast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grate carrot green appl middl zest juic halv c...</td>\n",
       "      <td>carrot piec appl piec orang piec raisin g hone...</td>\n",
       "      <td>childhood breakfast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mix egg piec loaf egg mixtur veget oil side</td>\n",
       "      <td>baton piec milk tablespoon chicken egg piec sa...</td>\n",
       "      <td>french crouton breakfast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boil egg chees tast</td>\n",
       "      <td>green salad bundl chicken egg piec tomato piec...</td>\n",
       "      <td>low breakfast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              text_stemmed_sentences  \\\n",
       "0  put cottag chees wide bowl add egg sugar flour...   \n",
       "1  rins buckwheat pour cup boil water salt cover ...   \n",
       "2  grate carrot green appl middl zest juic halv c...   \n",
       "3        mix egg piec loaf egg mixtur veget oil side   \n",
       "4                                boil egg chees tast   \n",
       "\n",
       "                        ingredient_stemmed_sentences    name_stemmed_sentences  \n",
       "0  chicken egg piec soft cottag chees g wheat flo...            breakfast lazi  \n",
       "1  buckwheat cereal cup chop parsley tast chop ci...           breek breakfast  \n",
       "2  carrot piec appl piec orang piec raisin g hone...       childhood breakfast  \n",
       "3  baton piec milk tablespoon chicken egg piec sa...  french crouton breakfast  \n",
       "4  green salad bundl chicken egg piec tomato piec...             low breakfast  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gerekli kütüphaneleri yükleyelim\n",
    "import pandas as pd\n",
    "\n",
    "# CSV dosyasını tekrar yükleyelim\n",
    "stemmed_df = pd.read_csv('stemmed_data.csv')\n",
    "\n",
    "# Dosyanın ilk 5 satırına bakalım\n",
    "stemmed_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03def0db-3eb9-4c11-b920-e94a22c5ba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cümle 1 - Stemmed: ['put', 'cottag', 'chee', 'wide', 'bowl', 'add', 'egg', 'sugar', 'flour', 'fork', 'mix', 'homogen', 'mass', 'turn', 'sticki', 'add', 'flour', 'half', 'tabl', 'small', 'amount', 'flour', 'lay', 'cottag', 'two', 'equal', 'part', 'roll', 'sausag', 'thick', 'sausag', 'small', 'ident', 'piec', 'sharp', 'desir', 'slightli', 'add', 'piec', 'give', 'round', 'small', 'pan', 'bring', 'water', 'lower', 'dumpl', 'boil', 'water', 'one', 'stir', 'slightli', 'slot', 'spoon', 'dumpl', 'come', 'surfac', 'plu', 'anoth', 'finish', 'dumpl', 'pan', 'plate', 'pour', 'jam', 'exampl', 'serv', 'hot', 'warm']\n",
      "Cümle 2 - Stemmed: ['rin', 'buckwheat', 'pour', 'cup', 'boil', 'water', 'salt', 'cover', 'cover', 'minut', 'buckwheat', 'eaten', 'lose', 'nutrit', 'better', 'requir', 'amount', 'wound', 'buckwheat', 'portion', 'plate', 'season', 'oliv', 'oil', 'soy', 'sauc', 'lemon', 'juic', 'chop', 'lemon', 'chop', 'green', 'chop', 'veget', 'bulgarian', 'pepper', 'carrot', 'pumpkin', 'radish', 'green', 'cocktail']\n",
      "Cümle 3 - Stemmed: ['grate', 'carrot', 'green', 'appl', 'middl', 'zest', 'juic', 'halv', 'cut', 'second', 'small', 'appl', 'carrot', 'orang', 'raisin', 'nut', 'favorit', 'season', 'juic', 'honey', 'add', 'cinnamon']\n",
      "Cümle 4 - Stemmed: ['mix', 'egg', 'piec', 'loaf', 'egg', 'mixtur', 'veget', 'oil', 'side']\n",
      "Cümle 5 - Stemmed: ['boil', 'egg', 'chee', 'tast']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# CSV dosyasını yükle\n",
    "stemmed_data = pd.read_csv('stemmed_data.csv')\n",
    "\n",
    "# Stemmer ve stopword listesi\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Ön işleme fonksiyonu\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = sentence.split()  # Boşluklara göre kelimeleri ayır\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Her satırı işle\n",
    "tokenized_corpus_stemmed = []\n",
    "\n",
    "for sentence in stemmed_data['text_stemmed_sentences']:\n",
    "    try:\n",
    "        stemmed_tokens = preprocess_sentence(sentence)\n",
    "        tokenized_corpus_stemmed.append(stemmed_tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Hata oluştu: {e}\")\n",
    "        tokenized_corpus_stemmed.append([])\n",
    "\n",
    "# İlk 5 sonucu yazdır\n",
    "for i in range(5):\n",
    "    print(f\"Cümle {i+1} - Stemmed: {tokenized_corpus_stemmed[i]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2cda729-dc3f-403a-b1d8-ad84f67313ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              text_stemmed_sentences  \\\n",
      "0  put cottag chees wide bowl add egg sugar flour...   \n",
      "1  rins buckwheat pour cup boil water salt cover ...   \n",
      "2  grate carrot green appl middl zest juic halv c...   \n",
      "3        mix egg piec loaf egg mixtur veget oil side   \n",
      "4                                boil egg chees tast   \n",
      "\n",
      "                        ingredient_stemmed_sentences    name_stemmed_sentences  \n",
      "0  chicken egg piec soft cottag chees g wheat flo...            breakfast lazi  \n",
      "1  buckwheat cereal cup chop parsley tast chop ci...           breek breakfast  \n",
      "2  carrot piec appl piec orang piec raisin g hone...       childhood breakfast  \n",
      "3  baton piec milk tablespoon chicken egg piec sa...  french crouton breakfast  \n",
      "4  green salad bundl chicken egg piec tomato piec...             low breakfast  \n"
     ]
    }
   ],
   "source": [
    "# CSV dosyasındaki ilk birkaç satırı kontrol edelim\n",
    "print(stemmed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb38cd96-b69c-45ed-bd42-7053437610c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abandon  abc  abdomen  abdomin  abelu  abil  abkhaz  abkhazian  abl  abluk  \\\n",
      "0        0    0        0        0      0     0       0          0    0      0   \n",
      "1        0    0        0        0      0     0       0          0    0      0   \n",
      "2        0    0        0        0      0     0       0          0    0      0   \n",
      "3        0    0        0        0      0     0       0          0    0      0   \n",
      "4        0    0        0        0      0     0       0          0    0      0   \n",
      "\n",
      "   ...  zukata  zuko  zvezda  zwill  zyren  ºc  ñora  λάχανο  ρύζι  ᵒs  \n",
      "0  ...       0     0       0      0      0   0     0       0     0   0  \n",
      "1  ...       0     0       0      0      0   0     0       0     0   0  \n",
      "2  ...       0     0       0      0      0   0     0       0     0   0  \n",
      "3  ...       0     0       0      0      0   0     0       0     0   0  \n",
      "4  ...       0     0       0      0      0   0     0       0     0   0  \n",
      "\n",
      "[5 rows x 11475 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Stemmed edilmiş metinlerin listesi, her bir cümleyi tokenlerden tekrar metne çeviriyoruz\n",
    "stemmed_texts = [' '.join(tokens) for tokens in tokenized_corpus_stemmed]\n",
    "\n",
    "# TF-IDF vektörleştirici başlatıyoruz\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF matrisini oluşturuyoruz (sparse matrix)\n",
    "tfidf_matrix = vectorizer.fit_transform(stemmed_texts)\n",
    "\n",
    "# Sparse matrisi pandas DataFrame'e dönüştürme\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# İlk birkaç satırı gösterelim (ilk 5 cümleyi)\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18e6b13a-ce96-4e7c-9fdc-7082303f2e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['put cottag chee wide bowl add egg sugar flour fork mix homogen mass turn sticki add flour half tabl small amount flour lay cottag two equal part roll sausag thick sausag small ident piec sharp desir slightli add piec give round small pan bring water lower dumpl boil water one stir slightli slot spoon dumpl come surfac plu anoth finish dumpl pan plate pour jam exampl serv hot warm',\n",
       " 'rin buckwheat pour cup boil water salt cover cover minut buckwheat eaten lose nutrit better requir amount wound buckwheat portion plate season oliv oil soy sauc lemon juic chop lemon chop green chop veget bulgarian pepper carrot pumpkin radish green cocktail',\n",
       " 'grate carrot green appl middl zest juic halv cut second small appl carrot orang raisin nut favorit season juic honey add cinnamon']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ön işlenmiş stemmed token listelerini tekrar metne çeviriyoruz\n",
    "stemmed_texts = [' '.join(tokens) for tokens in tokenized_corpus_stemmed]\n",
    "\n",
    "# İlk 3 stemlenmiş metni yazdıralım\n",
    "stemmed_texts[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2183f3c1-372d-459c-9007-4ece170af36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abandon  abc  abdomen  abdomin  abelu  abil  abkhaz  abkhazian  abl  abluk  \\\n",
      "0      0.0  0.0      0.0      0.0    0.0   0.0     0.0        0.0  0.0    0.0   \n",
      "1      0.0  0.0      0.0      0.0    0.0   0.0     0.0        0.0  0.0    0.0   \n",
      "2      0.0  0.0      0.0      0.0    0.0   0.0     0.0        0.0  0.0    0.0   \n",
      "3      0.0  0.0      0.0      0.0    0.0   0.0     0.0        0.0  0.0    0.0   \n",
      "4      0.0  0.0      0.0      0.0    0.0   0.0     0.0        0.0  0.0    0.0   \n",
      "\n",
      "   ...  zukata  zuko  zvezda  zwill  zyren   ºc  ñora  λάχανο  ρύζι   ᵒs  \n",
      "0  ...     0.0   0.0     0.0    0.0    0.0  0.0   0.0     0.0   0.0  0.0  \n",
      "1  ...     0.0   0.0     0.0    0.0    0.0  0.0   0.0     0.0   0.0  0.0  \n",
      "2  ...     0.0   0.0     0.0    0.0    0.0  0.0   0.0     0.0   0.0  0.0  \n",
      "3  ...     0.0   0.0     0.0    0.0    0.0  0.0   0.0     0.0   0.0  0.0  \n",
      "4  ...     0.0   0.0     0.0    0.0    0.0  0.0   0.0     0.0   0.0  0.0  \n",
      "\n",
      "[5 rows x 11475 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Token listelerini tekrar düz metne çeviriyoruz (önceden yapılmadıysa)\n",
    "stemmed_texts = [' '.join(tokens) for tokens in tokenized_corpus_stemmed]\n",
    "\n",
    "# TF-IDF vektörizerı başlatıyoruz\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF matrisini oluşturuyoruz\n",
    "tfidf_matrix = vectorizer.fit_transform(stemmed_texts)\n",
    "\n",
    "# TF-IDF işleminde kullanılan tüm kelimelerin eşsiz bir listesini alıyoruz\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# TF-IDF matrisini pandas DataFrame'e çeviriyoruz\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# İlk birkaç satırı gösteriyoruz\n",
    "print(tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d5c96dc-3798-4254-94bd-7fa679426063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime (stemmed):\n",
      "dumpl     0.501643\n",
      "sausag    0.250943\n",
      "cottag    0.216644\n",
      "plu       0.203906\n",
      "flour     0.196647\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# İlk cümle için TF-IDF skorlarını alıyoruz\n",
    "first_sentence_vector_stemmed = tfidf_df.iloc[0]\n",
    "\n",
    "# Skorlara göre büyükten küçüğe sıralayıp ilk 5 kelimeyi alıyoruz\n",
    "top_5_words_stemmed = first_sentence_vector_stemmed.sort_values(ascending=False).head(5)\n",
    "\n",
    "# Sonuçları yazdırıyoruz\n",
    "print(\"İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime (stemmed):\")\n",
    "print(top_5_words_stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffe5730a-10a7-44a8-b668-6610c3129ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'potato' kelimesine en yakın 5 kelime (cosine similarity):\n",
      "potato: 1.0000\n",
      "mash: 0.3086\n",
      "salt: 0.2579\n",
      "boil: 0.2569\n",
      "onion: 0.2541\n",
      "cut: 0.2487\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "target_word = 'potato'\n",
    "\n",
    "# Kelimenin indeksini güvenli şekilde bulalım\n",
    "if target_word in feature_names:\n",
    "    target_index = feature_names.tolist().index(target_word)\n",
    "\n",
    "    # TF-IDF matrisinden kelimenin sütun vektörünü al (2D formatta)\n",
    "    target_vector = tfidf_matrix[:, target_index].toarray()\n",
    "\n",
    "    # Tüm kelimelerin TF-IDF vektörlerini alıyoruz (her sütun bir kelime)\n",
    "    tfidf_vectors = tfidf_matrix.toarray()\n",
    "\n",
    "    # Cosine similarity hesapla (kelime vektörü ile tüm kelimeler)\n",
    "    similarities = cosine_similarity(target_vector.T, tfidf_vectors.T)\n",
    "\n",
    "    # En yüksek 6 benzerliği al (kendisi dahil)\n",
    "    similarities = similarities.flatten()\n",
    "    top_5_indices = similarities.argsort()[-6:][::-1]\n",
    "\n",
    "    # Sonuçları yazdır\n",
    "    print(f\"'{target_word}' kelimesine en yakın 5 kelime (cosine similarity):\")\n",
    "    for idx in top_5_indices:\n",
    "        print(f\"{feature_names[idx]}: {similarities[idx]:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"'{target_word}' kelimesi TF-IDF kelime listesinde bulunamadı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226c0a17-3630-4ed1-a1b5-b013cf1706db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['potato', 'are', 'healthi', 'food'], ['tomato', 'are', 'a', 'kind', 'of', 'fruit'], ['potato', 'and', 'tomato', 'are', 'plant']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "corpus = [\n",
    "    \"Potatoes are healthy food.\",\n",
    "    \"Tomatoes are a kind of fruit.\",\n",
    "    \"Potatoes and tomatoes are plants.\"\n",
    "]\n",
    "\n",
    "# Tokenize ve stemleme\n",
    "tokenized_corpus_stemmed = []\n",
    "for doc in corpus:\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    stemmed_tokens = [ps.stem(token) for token in tokens if token.isalpha()]  # sadece alfabe karakterleri\n",
    "    tokenized_corpus_stemmed.append(stemmed_tokens)\n",
    "\n",
    "print(tokenized_corpus_stemmed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a80ddeb9-f037-4a6e-822e-8a6e913eb890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrisi başarıyla 'tfidf_matrix_stemmed.csv' dosyasına kaydedildi!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stemmed_texts = [' '.join(tokens) for tokens in tokenized_corpus_stemmed]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(stemmed_texts)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "df_tfidf.to_csv('tfidf_matrix_stemmed.csv', index=False)\n",
    "\n",
    "print(\"TF-IDF matrisi başarıyla 'tfidf_matrix_stemmed.csv' dosyasına kaydedildi!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9271350-d8f3-4613-98a9-16ce0f38ac5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
